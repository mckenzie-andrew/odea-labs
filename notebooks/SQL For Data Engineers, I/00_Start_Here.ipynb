{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875f94b8",
   "metadata": {},
   "source": [
    "# Welcome to the Data Engineering Lab\n",
    "Run the cells below to generate the datasets needed for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import duckdb\n",
    "from sqlalchemy import create_engine, text\n",
    "import opendatasets as od \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = \"/home/jovyan/data\"\n",
    "\n",
    "# Postgres Credentials\n",
    "PG_CONFIG = {\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"password\", \n",
    "    \"host\": \"postgres\", \n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"postgres\" \n",
    "}\n",
    "\n",
    "# URLs\n",
    "OLIST_URL = \"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\"\n",
    "NYC_TAXI_URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "def get_postgres_engine():\n",
    "    url = f\"postgresql://{PG_CONFIG['user']}:{PG_CONFIG['password']}@{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}\"\n",
    "    return create_engine(url)\n",
    "\n",
    "# --- 1. DEFINE SCHEMA ---\n",
    "DDL_STATEMENTS = \"\"\"\n",
    "-- Clean Slate\n",
    "DROP TABLE IF EXISTS order_items CASCADE;\n",
    "DROP TABLE IF EXISTS orders CASCADE;\n",
    "DROP TABLE IF EXISTS products CASCADE;\n",
    "DROP TABLE IF EXISTS customers CASCADE;\n",
    "DROP TABLE IF EXISTS sellers CASCADE;\n",
    "\n",
    "-- Create Tables\n",
    "CREATE TABLE customers (\n",
    "    customer_id VARCHAR(32) PRIMARY KEY,\n",
    "    customer_unique_id VARCHAR(32) NOT NULL,\n",
    "    customer_zip_code_prefix VARCHAR(10),\n",
    "    customer_city VARCHAR(100),\n",
    "    customer_state VARCHAR(5)\n",
    ");\n",
    "\n",
    "CREATE TABLE sellers (\n",
    "    seller_id VARCHAR(32) PRIMARY KEY,\n",
    "    seller_zip_code_prefix VARCHAR(10),\n",
    "    seller_city VARCHAR(100),\n",
    "    seller_state VARCHAR(5)\n",
    ");\n",
    "\n",
    "CREATE TABLE products (\n",
    "    product_id VARCHAR(32) PRIMARY KEY,\n",
    "    product_category_name VARCHAR(100),\n",
    "    product_weight_g INT,\n",
    "    product_length_cm INT,\n",
    "    product_height_cm INT,\n",
    "    product_width_cm INT\n",
    ");\n",
    "\n",
    "CREATE TABLE orders (\n",
    "    order_id VARCHAR(32) PRIMARY KEY,\n",
    "    customer_id VARCHAR(32) REFERENCES customers(customer_id),\n",
    "    order_status VARCHAR(20),\n",
    "    order_purchase_timestamp TIMESTAMP,\n",
    "    order_approved_at TIMESTAMP,\n",
    "    order_delivered_carrier_date TIMESTAMP,\n",
    "    order_delivered_customer_date TIMESTAMP,\n",
    "    order_estimated_delivery_date TIMESTAMP\n",
    ");\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "    order_id VARCHAR(32) REFERENCES orders(order_id),\n",
    "    order_item_id INT, \n",
    "    product_id VARCHAR(32) REFERENCES products(product_id),\n",
    "    seller_id VARCHAR(32) REFERENCES sellers(seller_id),\n",
    "    shipping_limit_date TIMESTAMP,\n",
    "    price DECIMAL(10,2),\n",
    "    freight_value DECIMAL(10,2),\n",
    "    PRIMARY KEY (order_id, order_item_id)\n",
    ");\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def setup_postgres_schema(engine):\n",
    "    print(\"--- üî® Building Postgres Schema ---\")\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(DDL_STATEMENTS))\n",
    "        conn.commit()\n",
    "    print(\"‚úÖ Schema created successfully.\")\n",
    "\n",
    "def setup_olist_data(engine):\n",
    "    print(\"\\n--- üõí Processing Olist Data ---\")\n",
    "    \n",
    "    target_folder = os.path.join(DATA_DIR, \"brazilian-ecommerce\")\n",
    "    if not os.path.exists(target_folder):\n",
    "        print(f\"Downloading Olist dataset...\")\n",
    "        od.download(OLIST_URL, data_dir=DATA_DIR)\n",
    "    \n",
    "    # Define explicitly which columns are allowed for each table\n",
    "    # This acts as a filter to drop the extra columns in the CSV\n",
    "    table_schemas = {\n",
    "        \"customers\": [\"customer_id\", \"customer_unique_id\", \"customer_zip_code_prefix\", \"customer_city\", \"customer_state\"],\n",
    "        \"sellers\": [\"seller_id\", \"seller_zip_code_prefix\", \"seller_city\", \"seller_state\"],\n",
    "        \"products\": [\"product_id\", \"product_category_name\", \"product_weight_g\", \"product_length_cm\", \"product_height_cm\", \"product_width_cm\"],\n",
    "        \"orders\": [\"order_id\", \"customer_id\", \"order_status\", \"order_purchase_timestamp\", \"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\"],\n",
    "        \"order_items\": [\"order_id\", \"order_item_id\", \"product_id\", \"seller_id\", \"shipping_limit_date\", \"price\", \"freight_value\"]\n",
    "    }\n",
    "\n",
    "    load_sequence = [\n",
    "        (\"olist_customers_dataset.csv\", \"customers\"),\n",
    "        (\"olist_sellers_dataset.csv\", \"sellers\"),\n",
    "        (\"olist_products_dataset.csv\", \"products\"),\n",
    "        (\"olist_orders_dataset.csv\", \"orders\"),\n",
    "        (\"olist_order_items_dataset.csv\", \"order_items\")\n",
    "    ]\n",
    "\n",
    "    for filename, table_name in load_sequence:\n",
    "        file_path = os.path.join(target_folder, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è Missing {filename}, skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading {table_name}...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 1. Rename Columns\n",
    "        rename_map = {\n",
    "            \"zip_code_prefix\": f\"{table_name[:-1]}_zip_code_prefix\" if table_name in ['customers', 'sellers'] else \"zip_code_prefix\",\n",
    "            \"city\": f\"{table_name[:-1]}_city\" if table_name in ['customers', 'sellers'] else \"city\",\n",
    "            \"state\": f\"{table_name[:-1]}_state\" if table_name in ['customers', 'sellers'] else \"state\"\n",
    "        }\n",
    "        \n",
    "        new_cols = {}\n",
    "        for col in df.columns:\n",
    "            if col in rename_map:\n",
    "                new_cols[col] = rename_map[col]\n",
    "            # Handle specific CSV vs SQL mismatches\n",
    "            elif col == 'zip_code_prefix' and table_name == 'customers': new_cols[col] = 'customer_zip_code_prefix'\n",
    "            elif col == 'city' and table_name == 'customers': new_cols[col] = 'customer_city'\n",
    "            elif col == 'state' and table_name == 'customers': new_cols[col] = 'customer_state'\n",
    "            elif col == 'zip_code_prefix' and table_name == 'sellers': new_cols[col] = 'seller_zip_code_prefix'\n",
    "            elif col == 'city' and table_name == 'sellers': new_cols[col] = 'seller_city'\n",
    "            elif col == 'state' and table_name == 'sellers': new_cols[col] = 'seller_state'\n",
    "\n",
    "        df.rename(columns=new_cols, inplace=True)\n",
    "\n",
    "        # 2. Date Cleaning\n",
    "        for col in df.columns:\n",
    "            if 'date' in col or 'timestamp' in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "        # 3. FILTERING (The Fix)\n",
    "        # We assume the columns match the names in our table_schemas.\n",
    "        # We select ONLY the columns that exist in our SQL definition.\n",
    "        valid_cols = table_schemas[table_name]\n",
    "        \n",
    "        # Check if any valid columns are missing from the dataframe (optional safety check)\n",
    "        missing_cols = [c for c in valid_cols if c not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"‚ö†Ô∏è Warning: Missing columns {missing_cols} in {filename}. Filling with NULL.\")\n",
    "            for c in missing_cols:\n",
    "                df[c] = None\n",
    "\n",
    "        # Keep only the valid columns\n",
    "        df = df[valid_cols]\n",
    "\n",
    "        # 4. Insert\n",
    "        try:\n",
    "            df.to_sql(table_name, engine, index=False, if_exists='append', chunksize=10000, method='multi')\n",
    "            print(f\"‚úÖ {table_name}: {len(df)} rows loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading {table_name}: {e}\")\n",
    "            # If a parent table fails, stop the script because children will fail too\n",
    "            raise e\n",
    "\n",
    "def setup_duckdb():\n",
    "    print(\"\\n--- üöñ Setting up DuckDB (NYC Taxi) ---\")\n",
    "    parquet_path = os.path.join(DATA_DIR, \"nyc_taxi_2023_01.parquet\")\n",
    "    if not os.path.exists(parquet_path):\n",
    "        print(\"Downloading NYC Taxi Parquet...\")\n",
    "        r = requests.get(NYC_TAXI_URL)\n",
    "        with open(parquet_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    db_path = os.path.join(DATA_DIR, \"analytics.duckdb\")\n",
    "    con = duckdb.connect(db_path)\n",
    "    con.execute(f\"CREATE OR REPLACE TABLE taxi_trips AS SELECT * FROM read_parquet('{parquet_path}');\")\n",
    "    count = con.execute(\"SELECT count(*) FROM taxi_trips\").fetchone()[0]\n",
    "    print(f\"‚úÖ DuckDB ready with {count:,} rows.\")\n",
    "    con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        os.makedirs(DATA_DIR)\n",
    "        \n",
    "    try:\n",
    "        pg_engine = get_postgres_engine()\n",
    "        setup_postgres_schema(pg_engine) \n",
    "        setup_olist_data(pg_engine)\n",
    "        setup_duckdb()\n",
    "        print(\"\\nüéâ LAB SETUP COMPLETE üéâ\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Setup Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9e3b4-3658-48ac-821e-7386989fe6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# --- 1. TEST POSTGRES (Relational Check) ---\n",
    "print(\"--- üêò Testing PostgreSQL Connection & Joins ---\")\n",
    "\n",
    "# Connect to the Postgres Container\n",
    "pg_engine = create_engine(\"postgresql://admin:password@postgres:5432/postgres\")\n",
    "\n",
    "# Run a query that requires JOINING tables\n",
    "# If this works, your Foreign Keys and Schema are perfect.\n",
    "sql_query = \"\"\"\n",
    "    SELECT \n",
    "        c.customer_state,\n",
    "        COUNT(o.order_id) as total_orders\n",
    "    FROM orders o\n",
    "    JOIN customers c ON o.customer_id = c.customer_id\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_pg = pd.read_sql(sql_query, pg_engine)\n",
    "    print(\"‚úÖ Postgres Query Successful! Top 5 States by Order Volume:\")\n",
    "    display(df_pg)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Postgres Test Failed: {e}\")\n",
    "\n",
    "# --- 2. TEST DUCKDB (Analytics Check) ---\n",
    "print(\"\\n--- ü¶Ü Testing DuckDB File & Aggregation ---\")\n",
    "\n",
    "# Connect to the persistent file we created\n",
    "db_path = \"/home/jovyan/data/analytics.duckdb\"\n",
    "\n",
    "try:\n",
    "    con = duckdb.connect(db_path)\n",
    "    \n",
    "    # Run a fast aggregation on the Taxi data\n",
    "    # We use .df() to return a Pandas DataFrame directly\n",
    "    df_duck = con.execute(\"\"\"\n",
    "        SELECT \n",
    "            count(*) as total_rides,\n",
    "            avg(total_amount) as avg_fare,\n",
    "            max(trip_distance) as max_distance\n",
    "        FROM taxi_trips\n",
    "    \"\"\").df()\n",
    "    \n",
    "    print(\"‚úÖ DuckDB Query Successful! Taxi Data Summary:\")\n",
    "    display(df_duck)\n",
    "    con.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå DuckDB Test Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b65b5d-9448-44c3-ac3d-caf34be9c0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
